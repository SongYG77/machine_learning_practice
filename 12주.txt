기계 12주차

RNN

뉴스 라벨링 실습

뉴스에 해당하는 문장을 각각 단어로 나눠 라벨링을 한다.

처음에 데이터를 부른다.
11258개의 뉴스 기사가 있다. 라벨은 46개

데이터가 너무 크기 때문에 numwords와 maxlen으로 크기를 지정해준다.

문장들은 항상 길이가 같지 않다. Y의 경우는 카테고리의 값이다.

문장의 길이가 달라 데이터의 크기가 같지 않기 때문에 패딩을 이용한다. max len으로 문장의 단어 개수?를 50으로 지정해놨다. 
패딩의 경우 keras모델에 있다.
from keras.preprocessing.sequence import pad_sequences
이것을 이용하여

x_train = pad_sequences(x_train, padding = 'post')
이것으로 채워지지 않은 부분을 0으로 채운다.


그러면 len의 길이가 같으므로 50개의 단어로 이루어진 같은 길이의 데이터로 맞춰진다.

그리고 x값을 reshape을 해준다. 
이제 y값을 만들어주는데
from keras.utils import to_categorical

y_data = to_categorical(y_data)
y값을 바꿔주는 건데 63개의 카테고리가 있을 때, y데이터에 있는 카테고리 값들을 1로 만들어준다. 즉 카테고리가 3이면  index 3번째 값에 1을 넣어주고, 나머지는 다 0이다, 두번째 데이터의 카테고리가 2면 그 다음줄(다음행?)의 인덱스 2번만 1이고 나머지는 다 0이다. 
예로 5개의 카테고리에 3개의 데이터라고 하면 y가
3
1
4
라고 할 때
00010
01000
00001
이렇게 y를 바꿔주는 것이다.


그러고 simple RNN 모델을 만들어준다.(이전시간과 같다) input_shape는 (49,1)형태이다.

카테고리들을 가지고 다루면서 제일 큰 값을 찾기 때문에 dense를 마지막에 하고 softmax활성화 함수로 최대값을 찾는다.


Stacked vanilla RNN
조금더 모델을 딥하게 층을 쌓는 것이다.
즉 출력부분 사이에 층이 더 생기는 것이다. 사용 방법은 레이어를 더 add해주면 된다.
model.add(SimpleRNN)
model.add(SimpleRNN)
model.add(SimpleRNN)
이런식으로 쌓는 것이다. 대신 이제 
레이어가 더 생겼기 때문에 결과를 넘겨줘야 한다. 그래서 return_sequences를 마지막 레이어를 제외하고 다 True 로 지정해줘야한다.


many to many
많은 입력과 그에 대한 많은 출력들
문장을 입력으로 받는데 이것 역시 문장의 길이가 같지 않다.

형태소 라벨링
큰 흐름
단어를 embedding해준다. 단어 그대로를 쓰는 것이 아니다. 임베딩 한 값을 x1으로 입력으로 들어간다. 우리가 알고 있는 하이퍼파라미터를 가지고 이전상태와 연산해 f값을 넘겨주는데 이것이 넘겨주기만 하는 것이 아닌 출력도 한다.  즉 넘겨주기도 하면서 결과를 출력하기도 한다. 이 동작을 각 단어마다 반복을 하는 것이다. 

우리는 로스를 원래 값과 비교하면서 출력을 바꿧는데 이번에는 출력이 여러개여서 로스값 역시 여러개를 가진다. 그래서 이 로스들의 평균을 구해서 최종 로스를 가지고 이 값을 줄여나간다. 

우리는 학습하기 전 문장의 길이를 맞추기 위해 패딩 처리를 했었다. 그런데 이 패딩들의 로스값은 데이터로 들어가면 안된다. 이 부분을 고려해야 로스를 줄일 수 있다.

시퀀스 마스킹.
우리가 패딩처리한 0값들을 무의미한 값이다라고 알려주는 부분이다. 이것을 마스킹이라고 한다.

keras.layer.Masking 레이어를 추가하고 
mask_zero =True로
keras.layer.Embedding 레이어를 구성합니다.

단어들의 리스트와 그에 상응하는 형태소 리스트를 두개 만든다.

단어들이 유니크값을 가지기 위해 set을 이용한다. 그리고 패딩을 이용하기 위해 '<pad>'값을 추가해 준다.

단어 리스트를 인덱스별로 매칭하기 위해서 딕셔너리를 만들고 
인덱스별 단어를 알기 위해서 반대로 인덱스별 단어에 대한 딕셔너리를 만든다.

이제 이 매칭된 인덱스(토큰값)들로 원래 데이터를 바꿔 x값으로 주고
형태소별로 정리된 값 역시 똑같이 토큰값으로 만들어준다. Y데이터.

패딩을 처리해준다. 그리고 패딩 처리했다는 것을 알려주기 위해 mask도 만들어준다.

x_data_mask = ((x_data != 0)*1).astype(float32)
이거 하면 값이 있는 부분은 1 패딩은 0으로 나온다.



x_data_len = list(map(lambda sentence : len(sentence), sentences))
padding을 제외한 각 값의 길이를 


y값을 카테고리컬 해준다.


이제 모델을 만드는 부분이다.
먼저 임베딩하는 부분이다.


model.add(Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True,
                               trainable=False, input_length=max_sequence,
                               embeddings_initializer=keras.initializers.Constant(one_hot)))

디멘션을 지정해준다. 미리 지정한 input과 output에 대한 디멘션을 넣어준다. 디멘션 설정은 word2idx 의 길이 즉 단어에 대해 토큰값 지정해준 걸 사용한다.
mask_zero = True로 마스크를 하겟다는 뜻.
즉 단어들의 전처리 부분이다.

SimpleRNN에서 units옵션값을 히든 디멘션, 즉 히든 디멘션만큼 만들것이다. 리턴 시퀀스 역시 트루로 해준다.
TimeDistributed()
각각의 스텝마다의 오류값을 계산해 업데이트하는 레이어다. dense를 사용한다. 매 스탭마다 계산을 해 하위 스텝에 영향을 주는 것이다.(역전파 이용)활성화 함수는 softmax이다.

 
단점:
입력 데이터가 커지면 학습 능력이 저하
데이터 뒤쪽으로 갈 수 록 앞쪽의 입력 데이터 정보 소실

입력데이터와 출력데이터 사이의 길이가 멀어질 수 록 연관관계가 줄어든다.

이 단점을 보완하기 위해 LSTM을 사용한다.

LSTM
히든 스테이트에 히든 셀 스테이트를 추가한 구조이다. 그래서 앞의 값이 뒤에 필요한 데이터인지 판단 한다.








