기계 13주차
RNN

로이터 뉴스 분류하기 - Vanilla RNN
Stacked Vanilla RNN 
이전시간 배운것.

RNN단점
입력 데이터가 커지면 학습능력 저하
데이터의 뒤쪽으로 갈 수록 앞쪽의 입력 데이터 정보 소실
입력 데이터와 출력 데이터 사이의 길이가 멀수록 연관관계가 줄어듦
(장기 의존성 문제)


LSTM
장기 의존성 문제 해결을 위해.
RNN의 히든 스테이트에 cell state를 추가한 것.

스테이트가 꽤 오래 경과하더라도 그래디언트가 잘 전파되도록.

LSTM
히든스테이트에 있는 셀스테이트 설명

@forget gate layer.
앞에 있는 레이어의 입력값이 정말 필요한지 결정하는 셀 (시그모이드 레이어)
ft = o(Wf * [ht-1,x] + bf)
o는 시그모이드 함수

@input gate layer
it = o(Wf * [ht-1,xt] + bi)
Ct = tanh(Wc* [ht-1,xt] + bc)
다 wx + b형태를 기본으로 함

이 두개에 대한 product를 이용한다. 그래서 현재 있는 입력을 기준으로 전 데이터가 얼마만큼의 영향을 주는지 결정.

forget과 input state에서 값을 이전값과 연산

forget레이어에서 0,1값을 넘겨줘서 이전 레이어에서 넘겨받은 값과 곱한다. 관련이 없다면 0이 곱해져 기여를 안하게 된다.

이후 input state에서 나온 값과 더한다.
업데이트를 얼마나 할지 결정

@output layer
input데이터를 태워서 cell state의 어느 부분을 output으로 내보낼지 결정. 시그모이드 레이어를 통해 나온 값이 ot
ot = o(Wo[ht-1,xt] + bo)
ht = ot * tanh(Ct)
여기까지가 LSTM의 기본 모델이다.


LSTM 변형 모델(GRU)
forget과 input이 합쳐진 LSTM이다.
Ct = ft * Ct-1 + (1 - ft)*Ct

입력이 긴 데이터이면 심플 RNN보다 LSTM이나 GRU를 이용하는 것이 좋다.



원핫인코딩
간단하게 피처 값을 유형에 따라 새로운 피처에 추가해 고유값에 해당하는 칼럼에만 1을 표시하고 나머지는 0을 표시하는 방법.


임베딩
범주형 자료를 연속성 벡터 형태로 변환
주로 인공신경망 학습을 통해 범주형 자료를 벡터 형태로 변환
t- SNE와 같은 방법 활용.

하는이유(짱점)
차원을 축소할 수 있다.
onehot 인코딩으로 표현할 경우 n-1개의 차원 필요 반면 임베딩은 2,3차원으로도 자료를 표현가능.


LSTM과 GRU 실습.
리뷰에 대해서 긍정이면 1 부정이면 0으로 표시.

러닝 데이터값이 너무 커질 경우 종료할수 있는 모듈이 EarlyStopping 이고 ModelCheckpoint 는 로스값이 내려가다 올라갈 경우가 있는데 내 현재값보다 로스가 작을때만 저장하도록 하는 것이다.
둘다 karas의 callbacks안에 있다.

먼저 패딩을 해 길이를 맞춘다
모델을 sequential한다.
모델에 임베딩을 추가한다. vocab size를 지정 임베딩 안에 숫자 100개를 만든다.
gru를 추가한다 GRU(128)에서 128은 레이어의 개수? 
마지막은 Dense를 사용하는데 활성화함수는 시그모이드이다.
EarlyStopping과 ModelCheckpoint를 추가하고 compile을 추가한다.

그리고 fit










