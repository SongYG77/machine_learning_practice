 기계 7주차

 weight 값 초기화
성능을 향상시킴
w값이 랜덤으로 지정되기 때문에 원하는 결과가 잘 나오지 않을 때도 있다.

그래서 어떻게 하면 w값을 잘 줄 수 있는지

로스를 줄이는 것이 목표이지만 
로스함수의 그래프는 항상 이쁜 2차원형태가 아니여서 
잘못된 값을 찾을 수 있다.

강의자료 코드 있음
이전까지의 코딩은 랜덤 값이엿다

이번엔 새로운 가중치 초기화
model.add(keras.layer.Dense(300, weight_init, activation="relu"))

코드에서 weight_init 부분이 가중치를 내가 정한 방법으로 하겟다는 파라미터값이다.

weight_init 값으로 
'glorot_uniform' : Xaiver이다.
'RandomNormal'
'he_normal'
세개를 넣었다. 
이게 가중치 초기화의 다른 방법들이다.

he가 가장 최근에 나온 것이고 relu와 사용할 시 좋다.

model.compile()의 옵션인
optimizer = ''
loss = ''

optimizer은 어떤 알고리즘을 사용할 것인지 
loss는 어떤 방법으로 loss를 찾을지 정해주는 옵션이다. 
각 부분에 알고리즘들이 많은데 강의자료에 있다.


=============================
Dropout 과 Batch Normalization

둘다 성능 향상에 도움이 된다.

과적합이 좋지 않은 이유
트레이닝에 대해서는 잘맞지만 테스트에서 값 이 오류인 값으로 판단하기 매우 쉽게 동작한다
강의자료의 그래프를 보면
train그래프가 test에 비해 너무 잘 맞는 것을 볼 수 있다.

이때 사용하는게 dropout
너무 과적합되지 않도록 일부의 뉴런들을 빼는 것을 의미한다.
(예로 흰색 티셔츠와 검은색 티셔츠가 있을 때 서로 같은 종류로 분류되기 위해 색상 뉴런을 빼는 정도)

model.add(keras.layer.Dense(300, weight_init, activation="relu"))
model.add(keras.layer.Dropout(0.3))

이렇게 사용한다.
 0.3은 퍼센트를 의미


Batch Normalization
각각의 스칼라 feature들을 독립적으로 정규화
레이어가 깊어 질수록  값들이 점점 찌그러진 그래프를 가지게 될 수 있는데 이를 정규 분포로 만들어 주기 위해.

각각의 feature들의 평균과 분포를 0과1로 정규화

model.add(keras.layer.Dense(300, weight_init, activation="relu"))
model.add(keras.layer.BatchNormalization)
